batch_size: 48
dropout: 0.2
epochs: 10
hidden_size: 256
learning_rate: 2.0e-05
num_layers: 1
